defaults:
  - _self_
  - augmentations: symmetric_auto_augment.yaml
  - wandb: private.yaml
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# disable hydra outputs
hydra:
  output_subdir: null
  run:
    dir: .

name: "(Group Minet) supcon (cifar_settings - resnet12_nc, proj_type: proj_nc, aa_string: rand-m9-mstd0.5-inc1, simclr_weight:0.1)" # change here to change name in wandb
method: "supcon"
backbone:
  # name: "resnet18_alice"
  name: "resnet12_nc"
method_kwargs:
  proj_hidden_dim: 2048
  proj_output_dim: 128
  temperature: 0.2 # 0.2 initially. 
  margin: 0
  proj_type: "proj_ncfscil"
  apply_infonce: True
  simclr_weight: 0.1
  convex_loss_comb: True
data:
  # dataset: cifar100 # change here for cifar100
  dataset: fscil_minet
  train_path: "/BS/fscil/work/datasets"
  val_path: "/BS/fscil/work/datasets"
  format: "image_folder"
  num_workers: 4
  aa_string: "rand-m9-mstd0.5-inc1" #rand-m9-mstd0.5-inc1, v0-mstd0.5
optimizer:
  name: "lars"
  batch_size: 512 #512
  lr: 0.8         #0.4
  classifier_lr: 0.1
  weight_decay: 0.00001
  kwargs:
    momentum: 0.9
    clip_lr: True
    eta: 0.02
    exclude_bias_n_norm: True
scheduler:
  name: "warmup_cosine"
  min_lr: 0
  warmup_start_lr: 0.003
  warmup_epochs: 10
checkpoint:
  enabled: True
  dir: "trained_models"
  frequency: -1
  keep_prev: False
auto_resume:
  enabled: False

# overwrite PL stuff
max_epochs: 300
devices: [0,1,2]
sync_batchnorm: True
accelerator: "gpu"
strategy: "ddp"
precision: 16
