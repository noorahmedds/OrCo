import argparse

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.resnet18_encoder import *
from models.resnet20_cifar import *
from models.resnet12_encoder import *
# import models.resnet18_lcwof as lcwof_net
from .helper import *
from .mixup import *

from tqdm import tqdm
from pdb import set_trace as bp

from utils import *

from torch.autograd import Variable

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.")

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return 

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class dino_MLP(nn.Module):
    def __init__(self, in_dim, out_dim, temp, num_layers=3, 
                 is_teacher = False, center_momentum = 0.9,
                 ncrops = 8, norm_last_layer = True, last_layer_type = "nll",
                 nepochs = 200, do_temp_schedule = False, warmup_teacher_temp = 0.04,  warmup_teacher_temp_epochs = 25):
        super().__init__()
        
        hidden_dim = 2048
        bottleneck_dim = 256
        
        # dino_outdim = out_dim   # number of classes the fixed fc classifies for. fc = (out_dim, dino_outdim)
        self.ncrops = ncrops
        

        self.num_layers = num_layers

        self.layer1 = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU()
        )
        self.layer2 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.GELU()
        )
        self.layer3 = nn.Sequential(
            nn.Linear(hidden_dim, bottleneck_dim),
        )
        
        self.apply(self._init_weights)

        if last_layer_type == "unit":
            self.create_fixed_unit_fc(bottleneck_dim, out_dim)
        elif last_layer_type == "etf":
            self.create_fixed_etf(bottleneck_dim, out_dim)
        elif last_layer_type == "nll":
            self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
            self.last_layer.weight_g.data.fill_(1)
            if norm_last_layer:
                self.last_layer.weight_g.requires_grad = False

        self.temp = temp
        # Teacher specific parameters
        self.is_teacher = is_teacher
        self.center_momentum = center_momentum
        self.register_buffer("center", torch.zeros(1, out_dim)) # Initialise center in the register buffer

        self.do_temp_schedule = do_temp_schedule
        if self.is_teacher and self.do_temp_schedule:
            self.teacher_temp_schedule = np.concatenate((
                np.linspace(warmup_teacher_temp,
                            self.temp, warmup_teacher_temp_epochs),
                np.ones(nepochs - warmup_teacher_temp_epochs) * self.temp
            ))

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def create_fixed_etf(self, bottleneck_dim, out_dim):
        # Setup etf layer
        orth_vec = generate_random_orthogonal_matrix(bottleneck_dim, out_dim)
        i_nc_nc = torch.eye(out_dim)
        one_nc_nc: torch.Tensor = torch.mul(torch.ones(out_dim, out_dim), (1 / out_dim))
        etf_vec = torch.mul(torch.matmul(orth_vec, i_nc_nc - one_nc_nc),
                            math.sqrt(out_dim / (out_dim - 1)))

        self.last_layer = nn.Linear(bottleneck_dim, out_dim, bias=False)
        self.last_layer.weight.data.copy_(etf_vec.T)
        self.last_layer.requires_grad = False

    def create_fixed_unit_fc(self, bottleneck_dim, out_dim):
        temp = torch.stack([torch.normal(torch.zeros(bottleneck_dim), torch.ones(bottleneck_dim)) for i in range(0, out_dim)])
        self.last_layer = nn.Linear(bottleneck_dim, out_dim, bias=False)
        self.last_layer.weight.data.copy_(temp)
        self.last_layer.requires_grad = False

    def update_center(self, teacher_output):
        """
        Update center used for teacher output.
        """

        # centering over the first dimension which is the batch. We will not need this for now. The center will be the current output
        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)
        # batch_center = teacher_output

        batch_center = batch_center / (len(teacher_output))

        # center update
        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)

    def forward(self, x, epoch):
        if self.do_temp_schedule:
            temp = self.teacher_temp_schedule[epoch]
        else:
            temp = self.temp

        if self.num_layers == 2:
            x = self.layer1(x)
            x = self.layer3(x)
        elif self.num_layers == 3:
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
        
        
        # l2 normalisation
        x = nn.functional.normalize(x, dim=-1, p=2)

        # normalized fully connected layer with K dimensions
        x = self.last_layer(x)
        
        # x = self.fixed_fc(x)

        # teacher centering and sharpening
        if self.is_teacher:
            # logit centering
            x = F.softmax((x - self.center) / temp, dim=-1)

            # Update teacher center
            self.update_center(x)
            x = x.detach()
        else:
            x = x / temp   # We skip softmax for student because that is directly handled in dino loss

        return x.chunk(self.ncrops)

class MYNET(nn.Module):

    def __init__(self, args, dino_args, mode=None, writer=None):
        super().__init__()

        self.mode = mode
        self.args = args
        
        if self.args.dataset in ['cifar100']:
            self.encoder = resnet20()
            self.num_features = 64
        if self.args.dataset in ['mini_imagenet']:
            self.encoder = resnet18(False, args)  # pretrained=False
            self.num_features = 512
            # Total trainable parameters: 11207232 
        if self.args.dataset == 'cub200':
            self.encoder = resnet18(True, args)  # pretrained=True follow TOPIC, models for cub is imagenet pre-trained. https://github.com/xyutao/fscil/issues/11#issuecomment-687548790
            self.num_features = 512

        self.writer = writer

        # self.cls_head = nn.Sequential(
        #     nn.Linear(self.num_features, 2048),
        #     nn.ReLU(),
        #     nn.Linear(2048, 512),
        #     nn.ReLU(),
        # )
        
        if self.args.classifier_last_layer == "projection":
            hidden_dim = self.args.classifier_hidden_dim
            self.fc = nn.Sequential(
                nn.Linear(self.num_features, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, self.num_features),
                nn.ReLU(),
                nn.Linear(self.num_features, self.args.num_classes, bias=False)       # Note the entire number of classes are already added 
            ) 
        elif self.args.classifier_last_layer == "linear":
            self.fc = nn.Linear(self.num_features, self.args.num_classes, bias=False)  

        if self.args.dino_over_fc:
            self.dino_indim = self.args.num_classes
        else:
            self.dino_indim = self.num_features

        # self.dino_outdim = 60000 # self.args.num_classes * 2 refer: https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/args.txt
        self.student = dino_MLP(self.dino_indim, self.args.dino_outdim, temp = self.args.student_temp, ncrops=dino_args.local_crops_number + 2)
        self.teacher = dino_MLP(self.dino_indim, self.args.dino_outdim, temp = self.args.teacher_temp, ncrops=2, is_teacher=True, nepochs=self.args.epochs_base, do_temp_schedule=self.args.teacher_scheduling, warmup_teacher_temp = self.args.warmup_teacher_temp, warmup_teacher_temp_epochs = self.args.warmup_teacher_temp_epochs)
        
        for p in self.teacher.parameters():
            p.requires_grad = False

        print(f"Student and Teacher are built.")

        self.base_avg_encoding = None

    def reset_fc(self, cuda = False):
        if cuda:
            self.fc = nn.Linear(self.num_features, self.args.num_classes, bias=False).cuda()       # Note the entire number of classes are already added
        else:
            self.fc = nn.Linear(self.num_features, self.args.num_classes, bias=False)

    def update_teacher(self, momentum_teacher=0.996):
        # EMA update for the teacher
        with torch.no_grad():
            m = momentum_teacher

            for param_q, param_k in zip(self.student.parameters(), self.teacher.parameters()):
                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)
    
    def forward_dino(self, crops, epoch):
        # crops is a list with 8 crops of the same image augmented

        # concatenate the crops which belong to the same resolution group
        # Pass them through the encoder
        idx_crops = torch.cumsum(torch.unique_consecutive(
            torch.tensor([inp.shape[-1] for inp in crops]),
            return_counts=True,
        )[1], 0)

        start_idx, output = 0, torch.empty(0).cuda()
        for end_idx in idx_crops:
            # _out = self.encode(torch.cat(crops[start_idx: end_idx]))
            _out = self.encode(torch.stack(crops[start_idx: end_idx]).cuda())
            
            # If ontop of fc layer then pass _out through the fc layer as well.
            if self.args.dino_over_fc:
                _out = self.fc(_out)

            # accumulate outputs
            output = torch.cat((output, _out))
            start_idx = end_idx

        # Pass both images through the same encoder and for teacher input select only 2 and clone.
        x1 = output
        x2 = output.clone()[:2].detach()

        # Now get the output probabilities from the student and teacher
        # We dont need intermediate outputs because we don't specifically need those. What in the end we need is just the fixed_fc dot product followed by softmaxed probabilities
        student_output = self.student(x1, epoch)
        teacher_output = self.teacher(x2, epoch)
        
        return student_output, teacher_output

    def forward_metric(self, x):
        x = self.encode(x)
        if 'cos' in self.mode:
            x = F.linear(F.normalize(x, p=2, dim=-1), F.normalize(self.fc.weight, p=2, dim=-1)) # Cosine classifier
            x = self.args.temperature * x
        elif 'dot' in self.mode:
            if self.args.cl2n:
                x = self.cl2n(x)
            x = self.fc(x)

        return x

    def encode(self, x):
        x = self.encoder(x)

        x = F.adaptive_avg_pool2d(x, 1)
        # x = self.avgpool(x)
        
        x = x.squeeze(-1).squeeze(-1)
        return x

    def forward_mixup(self, input, **kwargs):
        out = self.encoder(input, **kwargs)
        
        # Unpacking from mixup
        x, y_a, y_b, lam = out

        x = F.adaptive_avg_pool2d(x, 1)
        x = x.squeeze(-1).squeeze(-1)

        if 'cos' in self.mode:
            x = F.linear(F.normalize(x, p=2, dim=-1), F.normalize(self.fc.weight, p=2, dim=-1)) # Cosine classifier
            x = self.args.temperature * x
        elif 'dot' in self.mode:
            x = self.fc(x)

        return x, y_a, y_b, lam

    def forward(self, input):
        if self.mode != 'encoder':
            input = self.forward_metric(input)
            return input
        elif self.mode == 'encoder':
            input = self.encode(input)
            return input
        else:
            raise ValueError('Unknown mode')
    
    def freeze_backbone(self):
        params_to_train = ['fc.weight']
        for name, param in self.named_parameters():
            # if name in params_to_train: 
            param.requires_grad = True if name in params_to_train else False

    def update_fc(self, trainloader, testloader, class_list, session):
        for batch in trainloader:
            # data, label = [_.cuda() for _ in batch]
            data, label, _ = batch
            data = data.cuda()
            label = label.cuda()
            data=self.encode(data).detach()

        if self.args.not_data_init:
            # >>> Setting a random set of parameters as the final classification layer for the novel classifier
            new_fc = nn.Parameter(torch.rand(len(class_list), self.num_features, device="cuda"),requires_grad=True)
            nn.init.kaiming_uniform_(new_fc, a=math.sqrt(5))
        else:
            new_fc = self.update_fc_avg(data, label, class_list)

        if 'ft' in self.args.new_mode:  # further finetune
            # self.update_fc_ft(new_fc,data,label,session)
            # Pass here also the testloader and args and session
            return self.update_fc_ft_novel(new_fc, trainloader, testloader, session)

    def update_fc_avg(self,data,label,class_list):
        """
            Using the exemplars available during training to instantiate the classifier for the novel setting
        """
        new_fc=[]
        for class_index in class_list:
            data_index=(label==class_index).nonzero().squeeze(-1)
            embedding=data[data_index]
            proto=embedding.mean(0)     # Mean along the 0th axis of all the embeddings for this particular class index
            new_fc.append(proto)
            self.fc.weight.data[class_index]=proto

        # Note the original protonet sums the latent vectors over the support set, and divides by the number of classes. Not by the number of data points
        
        new_fc=torch.stack(new_fc,dim=0)
        return new_fc

    def get_logits(self,x,fc):
        if 'dot' in self.args.new_mode:
            return F.linear(x,fc)
        elif 'cos' in self.args.new_mode:
            return self.args.temperature * F.linear(F.normalize(x, p=2, dim=-1), F.normalize(fc, p=2, dim=-1))

    def update_fc_ft(self,new_fc,data,label,session):
        new_fc=new_fc.clone().detach()
        new_fc.requires_grad=True
        optimized_parameters = [{'params': new_fc}]
        optimizer = torch.optim.SGD(optimized_parameters,lr=self.args.lr_new, momentum=0.9, dampening=0.9, weight_decay=0)

        with torch.enable_grad():
            tqdm_gen = tqdm(range(self.args.epochs_new))
            for epoch in tqdm_gen:
                old_fc = self.fc.weight[:self.args.base_class + self.args.way * (session - 1), :].detach()
                fc = torch.cat([old_fc, new_fc], dim=0)
                logits = self.get_logits(data,fc)

                loss = F.cross_entropy(logits, label) # Technically this is base normalised cross entropy. Because denominator has base sum and targets are only for the novel classes

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                pass

        self.fc.weight.data[self.args.base_class + self.args.way * (session - 1):self.args.base_class + self.args.way * session, :].copy_(new_fc.data)

    def get_optimizer_new(self, optimized_parameters):
        if self.args.optimizer == "sgd":
            optimizer = torch.optim.SGD(optimized_parameters,lr=self.args.lr_new, momentum=0.9, dampening=0.9, weight_decay=self.args.decay_new)
        elif self.args.optimizer == "adam":
            optimizer = torch.optim.Adam(optimized_parameters, lr=self.args.lr_new, weight_decay=self.args.decay_new)
        elif self.args.optimizer == "adamw":
            optimizer = torch.optim.AdamW(optimized_parameters, self.args.lr_new, weight_decay=self.args.decay_new)
        return optimizer

    def cl2n(self, encoding):
        out = encoding - self.base_avg_encoding
        return F.normalize(out, p=2, dim=-1)

    def update_fc_ft_novel(self, new_fc, trainloader, testloader, session):
        # new_fc=new_fc.clone().detach()
        new_fc=new_fc.detach().clone()
        new_fc.requires_grad=True

        if self.args.dino_novel:
            # make multi opt
            dino_params = list(self.encoder.parameters()) + list(self.student.parameters())
            cls_optimizer = torch.optim.SGD([{'params': new_fc}], lr=self.args.lr_new, momentum=0.9, nesterov=True, weight_decay=self.args.decay)
            dino_optimizer = torch.optim.SGD(dino_params, lr=3e-6)
            optimizer = MultipleOptimizer(cls_optimizer, dino_optimizer)
        else:
            optimized_parameters = [{'params': new_fc}]
            optimizer = self.get_optimizer_new(optimized_parameters)

        criterion = nn.CrossEntropyLoss(label_smoothing = self.args.label_smoothing_novel)

        best_loss = None
        best_acc = None
        best_hm = None

        best_fc = None
        label_offset = 0
        if self.args.base_only:
            # Remove the logits between the novel and base classes
            # or completely 0 them out.
            label_offset = (session - 1) * self.args.way

        with torch.enable_grad():
            tqdm_gen = tqdm(range(self.args.epochs_novel))
            if self.args.base_only:
                old_fc = self.fc.weight[:self.args.base_class, :].detach()    
            else:
                old_fc = self.fc.weight[:self.args.base_class + self.args.way * (session - 1), :].detach()

            for epoch in tqdm_gen:
                total_loss = 0
                ta = Averager()
                # for i, batch in enumerate(tqdm_gen, 1):
                for data, label, all_crops in trainloader:
                    concat_fc = torch.cat([old_fc, new_fc], dim=0)
                    data = data.cuda()
                    label = label.cuda()
                    label[label >= self.args.base_class] -= label_offset    # Offsetting the label to match the fc ouptut

                    if self.args.mixup_novel:
                        # data will now contain the mixed up novel session data
                        data, targets_a, targets_b, lam = mixup_data(data, label, alpha = self.args.mixup_alpha)
                        data, targets_a, targets_b = map(Variable, (data, targets_a, targets_b))

                    encoding = self.encode(data).detach() #<<< The encoder is essentially frozen
                    if self.args.cl2n:
                        encoding = self.cl2n(encoding)

                    logits = self.get_logits(encoding, concat_fc)

                    if self.args.mixup_novel:
                        # loss = mixup_criterion(F.cross_entropy, logits, targets_a, targets_b, lam)
                        loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)
                    else:
                        # The loss now in the denominator accounts for all the novel classes + base classes
                        # loss = F.cross_entropy(logits, label) # Note, no label smoothing here
                        loss = criterion(logits, label)
                    
                    # DINO Loss
                    if self.args.dino_novel:
                        dino_loss = 0
                        if self.args.all_crops:
                            ixs = list(range(data.shape[0]))
                        else:
                            ixs = [random.randint(0,data.shape[0]-1)]   
                        for j in ixs:
                            crops = []
                            for v in all_crops:
                                crops.append(v[j])
                            student_output, teacher_output = self.forward_dino(crops, epoch)
                            dino_loss += compute_dino_loss(student_output, teacher_output)
                            del student_output, teacher_output
                        dino_loss = dino_loss / len(ixs)
                        dino_loss *= self.args.dino_loss_weight
                        # loss += dino_loss
                    
                    ta.add(count_acc(logits, label))
                    optimizer.zero_grad()
                    if self.args.dino_novel:
                        loss.backward(retain_graph = True)
                        dino_loss.backward()
                    else:
                        loss.backward()
                    optimizer.step()

                    total_loss += loss.item()

                # Model Saving
                if self.args.validation_metric_novel == "none":
                    out_string = '(Novel) Session {}, current_loss {:.3f}, train_acc {:.3f}'\
                        .format(session, total_loss, float('%.3f' % (ta.item() * 100.0)),)
                    # best_fc = new_fc.clone()
                    best_fc = new_fc.detach().clone()
                else:
                    vl, va, vaNovel, vaBase, vhm, vam = self.test_fc(concat_fc, testloader, epoch, session)
                    if self.args.validation_metric_novel == "hm":
                        # Validation
                        if best_hm is None or vhm > best_hm:
                            best_hm = vhm
                            # best_fc = new_fc.clone()
                            best_fc = new_fc.detach().clone()
                        out_string = '(Novel) Sess: {}, loss {:.3f}, trainAcc {:.3f}, testAcc {:.3f}, bestHM {:.3f}'\
                            .format(
                                session,
                                total_loss,
                                float('%.3f' % (ta.item() * 100.0)),
                                float("%.3f" % (va * 100.0)),
                                float("%.3f" % (best_hm * 100.0)),)
                    elif self.args.validation_metric_novel == "loss":
                        if best_loss is None or vl < best_loss:
                            best_loss = vl
                            # best_fc = new_fc.clone()
                            best_fc = new_fc.detach().clone()
                        out_string = '(Novel) Session {}, current_loss {:.3f}, train_acc {:.3f}'\
                            .format(session, total_loss,float('%.3f' % (ta.item() * 100.0)),)
                    elif self.args.validation_metric_novel == "acc":
                        if best_acc is None or va > best_acc:
                            best_acc = va
                            # best_fc = new_fc.clone()
                            best_fc = new_fc.detach().clone()
                        out_string = '(Novel) Session {}, current_loss {:.3f}, train_acc {:.3f}'\
                            .format(session, total_loss,float('%.3f' % (ta.item() * 100.0)),)

                tqdm_gen.set_description(out_string)

                self.writer.add_scalars(f"(Novel) Session {session} Training Graph", {
                    "nta": ta.item(),
                    "nva": 0,
                    # "nvaNovel": vaNovel
                }, epoch)
        
        # Update the weights
        # Deprecated
        # self.fc.weight.data[self.args.base_class + self.args.way * (session - 1):self.args.base_class + self.args.way * session, :].copy_(best_fc.data)
        # with torch.no_grad():
        # Updating the fc layer
        if self.args.base_only:
            self.fc.weight.data[self.args.base_class + self.args.way * (session - 1):self.args.base_class + self.args.way * session, :].copy_(best_fc.data)
        else:
            with torch.no_grad(): self.fc.weight[:self.args.base_class + self.args.way * session, :] = nn.Parameter(torch.cat([old_fc, best_fc]))


    def update_fc_joint(self,jointloader, testloader, class_list, session):
        # if 'ft' in self.args.new_mode:  # further finetune
        #     # self.update_fc_ft(new_fc,data,label,session)
        return self.update_fc_ft_joint(jointloader, testloader, class_list, session)

    def update_fc_ft_joint(self, jointloader, testloader, class_list, session):
        """
            Extract the parameters associated with the given class list.
            So only base classifier and the novel classifier
            Creating a new classifier for the current classes and the new novel classes
            Note that this new classifier is a contiguous classifier so it needs to be trained with taregts greater 
            than 60 to be offset by the number of novel classes we have seen
        """

        novel_ix_start = self.args.base_class + self.args.way * (session - 1)
        novel_ix_end = self.args.base_class + self.args.way * (session)

        # Create a base fc and novel fc and copy these into the old fc to be trained together
        # By creating separate classifier we ensure that the intermediate fcs donot effect the output for the joint dataset
        base_fc = nn.Parameter(torch.zeros(self.args.base_class, self.num_features, device="cuda"), requires_grad=True)
        new_fc = nn.Parameter(torch.zeros(self.args.way, self.num_features, device="cuda"), requires_grad=True)
        inter_fc = self.fc.weight[self.args.base_class:self.args.base_class + (self.args.way * (session-1)), :].detach()

        # Set the values for the base and new fc from the model fc
        base_fc.data.copy_(self.fc.weight.data[:self.args.base_class, :])
        new_fc.data.copy_(self.fc.weight.data[novel_ix_start:novel_ix_end, :])

        if self.args.dino_joint:
            # make multi opt
            dino_params = list(self.encoder.parameters()) + list(self.student.parameters())
            cls_optimizer = torch.optim.SGD([base_fc, new_fc], lr=self.args.lr_new, momentum=0.9, nesterov=True, weight_decay=self.args.decay)
            dino_optimizer = torch.optim.SGD(dino_params, lr=3e-6)
            optimizer = MultipleOptimizer(cls_optimizer, dino_optimizer)
        else:
            optimized_parameters = [base_fc, new_fc]
            optimizer = self.get_optimizer_new(optimized_parameters)

        criterion = nn.CrossEntropyLoss(label_smoothing = self.args.label_smoothing_joint)

        best_loss = None
        best_hm = None
        best_acc = None
        hm_patience = 15
        hm_patience_count = 0

        best_fc = None
        label_offset = 0
        if self.args.base_only:
            # Remove the logits between the novel and base classes
            # or completely 0 them out.
            label_offset = (session - 1) * self.args.way

        with torch.enable_grad():
            tqdm_gen = tqdm(range(self.args.epochs_joint))
            for epoch in tqdm_gen:
                total_loss = 0
                ta = Averager()
                # for i, batch in enumerate(tqdm_gen, 1):
                for data, label, all_crops in jointloader:
                    if self.args.base_only:
                        concat_fc = torch.cat([base_fc, new_fc], dim = 0)
                    else:
                        concat_fc = torch.cat([base_fc, inter_fc, new_fc], dim=0)

                    data = data.cuda()
                    label = label.cuda()
                    label[label >= self.args.base_class] -= label_offset

                    if self.args.mixup_joint:
                        data, targets_a, targets_b, lam = mixup_data(data, label, alpha = self.args.mixup_alpha)
                        data, targets_a, targets_b = map(Variable, (data, targets_a, targets_b))

                    encoding = self.encode(data).detach() #<<< The encoder is essentially frozen
                    if self.args.cl2n:
                        encoding = self.cl2n(encoding)

                    logits = self.get_logits(encoding, concat_fc)
                    
                    # label = label - label_offset # Offset labels only for labels with value greater than base_class
                    # label[label >= self.args.base_class] -= label_offset

                    if self.args.mixup_joint:
                        # loss = mixup_criterion(F.cross_entropy, logits, targets_a, targets_b, lam)
                        loss = mixup_criterion(criterion, logits, targets_a, targets_b, lam)
                    else:
                        # The loss now in the denominator accounts for all the novel classes + base classes
                        # loss = F.cross_entropy(logits, label) # Note, no label smoothing here
                        loss = criterion(logits, label) # Note, no label smoothing here

                    # DINO Loss
                    if self.args.dino_joint:
                        dino_loss = 0
                        if self.args.all_crops:
                            ixs = list(range(data.shape[0]))
                        else:
                            ixs = [random.randint(0,data.shape[0]-1)]   
                        for j in ixs:
                            crops = []
                            for v in all_crops:
                                crops.append(v[j])
                            student_output, teacher_output = self.forward_dino(crops, epoch)
                            dino_loss += compute_dino_loss(student_output, teacher_output)
                            del student_output, teacher_output
                        dino_loss = dino_loss / len(ixs)
                        dino_loss *= self.args.dino_loss_weight
                        # loss += dino_loss
                    
                    ta.add(count_acc(logits, label))
                    optimizer.zero_grad()
                    if self.args.dino_joint:
                        loss.backward(retain_graph = True)
                        dino_loss.backward()
                    else:
                        loss.backward()
                    # torch.nn.utils.clip_grad_norm_(self.model.parameters(), opt.clip)
                    optimizer.step()

                    total_loss += loss.item()

                # Model Saving
                vl, va, vaNovel, vaBase, vhm, vam = self.test_fc(concat_fc, testloader, epoch, session)
                if self.args.validation_metric == "hm":
                    # Validation
                    if best_hm is None or vhm > best_hm:
                        best_hm = vhm
                        # best_fc = concat_fc.clone()
                        best_fc = concat_fc.detach().clone()
                        hm_patience_count = 0
                    else:
                        hm_patience_count += 1
                    out_string = '(Joint) Sess: {}, loss {:.3f}, trainAcc {:.3f}, testAcc {:.3f}, bestHM {:.3f}'\
                        .format(
                            session, 
                            total_loss,
                            float('%.3f' % (ta.item() * 100.0)),
                            float("%.3f" % (va * 100.0)),
                            float("%.3f" % (best_hm * 100.0)),)
                elif self.args.validation_metric == "loss":
                    if best_loss is None or vl < best_loss:
                        best_loss = vl
                        # best_fc = concat_fc.clone()
                        best_fc = concat_fc.detach().clone()
                    out_string = 'Session {}, current_loss {:.3f}, train_acc {:.3f}'\
                        .format(session, total_loss,float('%.3f' % (ta.item() * 100.0)),)
                elif self.args.validation_metric == "acc":
                    if best_acc is None or va > best_acc:
                        best_acc = va
                        # best_fc = concat_fc.clone()
                        best_fc = concat_fc.detach().clone()
                    out_string = 'Session {}, current_loss {:.3f}, train_acc {:.3f}'\
                        .format(session, total_loss,float('%.3f' % (ta.item() * 100.0)),)
                elif self.args.validation_metric == "acc+hm":
                    vahm = (va + vhm) / 2.0
                    if best_acc is None or vahm > best_acc:
                        best_acc = vahm
                        best_fc = concat_fc.detach().clone()
                    out_string = '(Joint) Sess: {}, loss {:.3f}, trainAcc {:.3f}, testAcc {:.3f}, bestHM {:.3f}'\
                        .format(
                            session, 
                            total_loss,
                            float('%.3f' % (ta.item() * 100.0)),
                            float("%.3f" % (va * 100.0)),
                            float("%.3f" % (best_hm * 100.0)),)

                tqdm_gen.set_description(out_string)

                self.writer.add_scalars(f"(Joint) Session {session} Training Graph", {
                    "jta": ta.item(),
                    "jva": va,
                    # "jvaNovel": vaNovel
                }, epoch)
                
                if hm_patience_count > hm_patience:
                    # faster joint session
                    break
        
        # Update the weights
        if self.args.base_only:
            self.fc.weight.data[self.args.base_class + self.args.way * (session - 1):self.args.base_class + self.args.way * session, :].copy_(best_fc.data[-self.args.way:, :])
            self.fc.weight.data[:self.args.base_class, :].copy_(best_fc.data[:self.args.base_class, :])
            # self.fc.weight.data[novel_ix_start:novel_ix_end, :].copy_(best_fc.data[novel_ix_start:novel_ix_end, :])
        else:
            with torch.no_grad(): self.fc.weight[:self.args.base_class + self.args.way * session, :] = nn.Parameter(best_fc)
        


    def test_fc(self, fc, testloader, epoch, session):
        """
            Get the testing score for the fc that is being currently trained
        """
        test_class = self.args.base_class + session * self.args.way     # Final class idx that we could be in the labels this session
        vl = Averager()
        va = Averager()

        # >>> Addition
        vaBase = Averager() # Averager for novel classes only        
        vaNovel = Averager() # Averager for novel classes only

        label_offset = 0
        if self.args.base_only:
            # Remove the logits between the novel and base classes
            # or completely 0 them out.
            label_offset = (session - 1) * self.args.way

        # test_fc = fc.clone().detach()
        self.eval()

        with torch.no_grad():
            # tqdm_gen = tqdm(testloader)
            for batch in testloader:
                data, test_label = [_.cuda() for _ in batch]
                test_label[test_label >= self.args.base_class] -= label_offset

                encoding = self.encode(data).detach()
                if self.args.cl2n:
                    encoding = self.cl2n(encoding)

                logits = self.get_logits(encoding, fc)
                # TODO: Instead of just taking the dot product during testing
                # We need to integrate the dot product as well as the l2 distance between the prototype and the feature extractor output

                logits = logits[:, :test_class]

                loss = F.cross_entropy(logits, test_label)
                acc = count_acc(logits, test_label)

                # >>> Addition
                novelAcc, baseAcc = count_acc_(logits, test_label, test_class, self.args)

                vaNovel.add(novelAcc)
                vaBase.add(baseAcc)
                vl.add(loss.item())
                va.add(acc)

            vl = vl.item()
            va = va.item()

            # >>> Addition 
            vaNovel = vaNovel.item()
            vaBase = vaBase.item()

        vhm = hm(vaNovel, vaBase)
        vam = am(vaNovel, vaBase)

        return vl, va, vaNovel, vaBase, vhm, vam